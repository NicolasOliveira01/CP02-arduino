{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3f1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a559a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x são as imagens (imagens de números) e y são os labels (a resposta, qual número representa a imagem)\n",
    "# x_train (imagens) e y_train (labels) treinam o algoritmo para ele aprender os padrões \n",
    "# x_test (imagens) e y_test (respostas corretas das imagens, não é a resposta do algoritmo) são usados para testar o algoritmo\n",
    "# mnist -> banco de dados de imagens de números escritos com a sua label (resposta correta)\n",
    "# mnist.load_data() -> carrega o banco de dados mnist e quando usado com x_train, y_train, x_test e y_test, ele atribui\n",
    "# imagens (x) e o labels (y) de treino e teste\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "# reshape -> função do numpy que transforma o array para o formato que a rede esperam (imagem, altura, largura, canal)\n",
    "# -1 -> o numpy calcula automaticamente o número de imagens que estão sendo importadas do mnist ( geralmente 60.000 para \n",
    "# x_train e 10.000 para x_test)\n",
    "# 28, 28 -> altura e largura da imagem (28x28 pixels), cada pixel é um número entre 0 e 255 (branco e preto)\n",
    "# 1 -> indica que a imagem é preto e branco (1 canal), se fosse colorida seriam 3 canais (RGB)\n",
    "# /255 -> normaliza os valores dos pixels (facilita o aprendizado), 0 é branco, 1 é preto e 0,5 é um cinza (meio termo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020e04b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicolas\\Downloads\\tensorflow\\.env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "# camada entrada:\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "# camada oculta:\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "# camada saída:\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "# Sequential -> forma de criar uma rede neural, onde as camadas são empilhadas uma em cima da outra, formando uma “pilha” \n",
    "# linear de processamento dos dados\n",
    "\n",
    "# camada de entrada: \n",
    "    # Conv2D (Convolucional 2D) -> processamento de imagens, onde a rede neural aprende a identificar padrões (números) nas imagens. \n",
    "    # 32 -> número de filtros que a camada vai aprender (32 padrões por imagem, como linha, borda, curva). \n",
    "    # (3, 3) -> tamanho do filtro que percorre toda a imagem para extrair padrões. \n",
    "    # activation='relu' -> define a função de ativação, no caso ReLU que ajuda a rede a aprender padrões complexos\n",
    "    # input_shape -> indica o formato da entrada, no caso (28, 28, 1) que é a imagem de entrada (altura, largura e canais)\n",
    "# camada oculta:\n",
    "    # 1 - MaxPooling2D -> adiciona uma camada de pooling (manter apenas as informações mais importantes na imagem), o que ajuda a \n",
    "    # evitar overfitting e acelera o treinamento\n",
    "    # (2,2) -> percorre a imagem com uma janela de 2x2 pixels, mantendo apenas o valor máximo de cada janela\n",
    "    # 2 - Conv2D -> usado novamente para aprender os novos padrões depois do MaxPooling2D, porém agora ele tem 64 filtros \n",
    "    # 3 - MaxPooling2D -> deixa a rede mais eficiente e foca ainda mais nos padrões essenciais\n",
    "    # 4 - Flatten -> Preparar os dados para a camada Dense, durante as camadas convolucionais e de pooling as saídas eram 3D (altura, largura e \n",
    "    # canais) e agora são 1D (dados achatados para uma sequência única de números, como um vetor) que é o formato que o Dense espera\n",
    "    # 5 - Dense ->  combinar e processar os padrões extraídos pela camada oculta e preparando os dados para a saída\n",
    "    # 128 -> número de neurônios que serão processados e cada um aprendendo um padrão diferente\n",
    "    # activation -> define a função de ativação em cada neurônio, no caso ReLU \n",
    "# camada de saída:\n",
    "    # Dense -> camada final que vai gerar a saída da rede neural, no caso 10 neurônios (um para cada número de 0 a 9), cada neurônio representa\n",
    "    # a probabilidade do número da imagem ser aquele número\n",
    "    # activation='softmax' -> transforma a saída desses 10 neurônios em probabilidades, assim o modelo escolhe o dígito com a maior probabilidade\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    ")\n",
    "# compile -> define (não chega a treinar) como o modelo vai aprender e avaliar o desempenho durante o treino e validação\n",
    "# optimizer='adam' -> atualiza os pesos e viés (usa o erro medido no loss) para que o algoritmo aprenda a fazer previsões com menos erros\n",
    "# loss='sparse_categorical_crossentropy' -> mede o erro do modelo para ajustar os pesos\n",
    "# metrics['accuracy'] -> mede a acurácia (monitoramento), porcentagem de previsões corretas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563f38d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9009 - loss: 0.3204 - val_accuracy: 0.9838 - val_loss: 0.0552\n",
      "Epoch 2/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9858 - loss: 0.0474 - val_accuracy: 0.9888 - val_loss: 0.0401\n",
      "Epoch 3/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9907 - loss: 0.0304 - val_accuracy: 0.9905 - val_loss: 0.0341\n",
      "Epoch 4/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9930 - loss: 0.0214 - val_accuracy: 0.9905 - val_loss: 0.0331\n",
      "Epoch 5/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9948 - loss: 0.0153 - val_accuracy: 0.9907 - val_loss: 0.0355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22b49505bb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "# fit -> treina o modelo com os dados de entrada\n",
    "# epochs=5 -> o modelo vai repetir 5 vezes \n",
    "# batch_size=32 -> ao invés de passar todos os dados de uma vez para o modelo, ele vai processar 32 exemplos por vez (treinar de forma mais eficiente)\n",
    "# validation_split=0.1 -> usa 10% do conjunto de treino para validar o modelo (durante o treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b769d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0338\n",
      "Acurácia nos dados de teste: 0.9914000034332275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Acurácia nos dados de teste: {test_acc}\")\n",
    "# evaluate -> passa os dados de teste e calcula o erro e a acurácia (usado depois que o modelo já foi treinado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a80692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de Treino: 60000 imagens, 60000 rótulos.\n",
      "Conjunto de Teste: 10000 imagens, 10000 rótulos.\n"
     ]
    }
   ],
   "source": [
    "num_train_images = len(x_train)\n",
    "# imagens no conjunto de treino\n",
    "num_train_labels = len(y_train)\n",
    "# rótulos no conjunto de treino\n",
    "num_test_images = len(x_test)\n",
    "# imagens no conjunto de teste\n",
    "num_test_labels = len(y_test)\n",
    "# rótulos no conjunto de treino\n",
    "\n",
    "# Exibir as quantidades\n",
    "print(f\"Conjunto de Treino: {num_train_images} imagens, {num_train_labels} rótulos.\")\n",
    "print(f\"Conjunto de Teste: {num_test_images} imagens, {num_test_labels} rótulos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13db9665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 9000\n",
    "test_image = x_test[index:index+1]\n",
    "# 3D (28, 28, 1) -> 28 de altura e largura em pixels e 1 seria em preto e branco, imagens coloridas em canal 3\n",
    "# 4D (n, 28, 28, 1) -> n é o batch_size (grupo de exemplos usados usado de uma vez só para treinar o modelo)\n",
    "# x_test é um array com várias imagens e se fizer x_test[index] vai retornar no formato 3D, porém o model.predict() espera no formato 4D\n",
    "# por isso está fazendo o [index:index+1] o que seria 9000:9001 (retorna 1 elemento começando pelo index 9000) e o resultado vai ser um 4D\n",
    "# no caso (1, 28, 28, 1)\n",
    "true_label = y_test[index]\n",
    "# y_test[index] -> o label da única imagem que vai ser usada no predict, o x_test e o y_test usados com o mesmo index retornam uma imagem \n",
    "# e a resposta dessa imagem \n",
    "\n",
    "# Fazer a predição com o modelo treinado\n",
    "predicted_probabilities = model.predict(test_image)\n",
    "# neste caso que usou o mnist, o modelo foi treinado para prever imagens com os dígitios de 0 a 9 (10 classes), na saída do modelo (Dense) definiu 10\n",
    "# neurônios (cada um calcula a probabilidade do input pertencer a aquela classe) e o resultado é um array com 10 valores\n",
    "# [0.01, 0.02, 0.00, 0.01, 0.00, 0.00, 0.01, 0.90, 0.02, 0.03] -> se o modelo ve uma imagem como 7\n",
    "predicted_label = predicted_probabilities.argmax()\n",
    "# retorna o índice do array com a maior probabilidade, no caso 7\n",
    "\n",
    "# Exibir a imagem e os resultados\n",
    "plt.imshow(x_test[index].reshape(28, 28), cmap='gray')\n",
    "# imshow -> exibir imagens em formato de array, 2D preto e branco e 3D colorido\n",
    "# x_test[index] -> está selecionando uma única imagem através do index\n",
    "# .reshape(28, 28) -> transforma em um array 2D\n",
    "# cmap='gray -> diz ao imshow para exibir imagens no tom cinza\n",
    "plt.title(f\"Rótulo Verdadeiro: {true_label}\\nRótulo Predito: {predicted_label}\")\n",
    "# true_label -> resposta da imagem escolhida \n",
    "# predicted_label -> a resposta que o algoritmo preveu\n",
    "plt.axis('off')\n",
    "# Esconde as marcações dos eixos X e Y (números, linhas e ticks ao redor da imagem)\n",
    "plt.show() \n",
    "# mostra resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
